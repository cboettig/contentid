---
output: 
  github_document:
    df_print: tibble
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
Sys.setenv("CONTENTURI_HOME"= tempdir())
```

# contentid

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/contentid)](https://CRAN.R-project.org/package=contentid)
[![R build status](https://github.com/cboettig/contentid/workflows/R-CMD-check/badge.svg)](https://github.com/cboettig/contenturi/actions)
<!-- badges: end -->


`contentid` seeks to facilitate reproducible workflows that involve external data files through the use of content identifiers. 


R users frequently write scripts which must load data from an external file -- a step which increases friction in reuse and creates a common failure point in reproducibility of the analysis later on. Reading a file directly from a URL is often preferable, since we don't have to worry about distributing the data seperately ourselves.  For example, an analysis might read in the famous CO2 ice core data directly from ORNL repository:

```{r}
co2 <- read.table("http://cdiac.ornl.gov/ftp/trends/co2/vostok.icecore.co2", 
                  col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

However, we know that data hosted at a given URL could change or dissappear, and not all data we want to work with is available at a URL to begin with.  Digital Object Identifiers (DOIs) were created to deal with these problems of 'link rot'.  Unfortunately, there is no straight forward and general way to read data directly from a DOI, (which almost always resolves to a human-readable webpage rather than the data itself), often apply to collections of files rather than individual source we want to read in our script, and we must frequently work with data that does not (yet) have a DOI. Registering a DOI for a dataset has gotten easier through repositories with simple APIs like Zenodo and figshare, but this is still an involved process and still leaves us without a mechanism to directly access the data.  

`contenturi` offers a complementary approach to addressing this challenge, which will work with data that has (or will later receive) a DOI, but also with arbitrary URLs or with local files. The basic idea is quite similar to referencing data by DOI: we first "register" an identifier, and then we use that identifier to retrieve the data in our scripts:

```{r}
library(contentid)
register("http://cdiac.ornl.gov/ftp/trends/co2/vostok.icecore.co2")
```

Registering the data returns an identifier that we can `resolve` in our scripts to later read in the file:

```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
co2_b <- read.table(co2_file, 
                    col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

We can confirm this is the same data of course:

```{r}
identical(co2, co2_b)
```

## How this works

As the identifier (`hash://sha256/...`) itself suggests, this is merely the SHA-256 hash of the requested file.  This means that unless the data at that URL changes, we will always get that same identifier back when we register that file.  If we have a copy of that data someplace else, we can verify it is indeed precisely the same data.  For instance, `contentid` includes a copy of this file as well.  Registering the local copy verifies that it indeed has the same hash:

```{r}
co2_file_c <- system.file("extdata", "vostok.icecore.co2", package = "contentid")
register(co2_file_c)
```

```{r}
query_sources("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
```


## Acknolwedgements

`contentid` is largely based on the design and implementation of <https://hash-archive.org>, and can interface with the <https://hash-archive.org> API or mimic it locally.  `contentid` also draws inspiration from [Preston](https://github.com/bio-guoda/preston), a biodiversity dataset tracker, and [Elton](https://github.com/globalbioticinteractions/elton), a command-line tool to update/clone, review and index existing species interaction datasets.
