---
documentclass: jss
author:
  - name: Carl Boettiger
    affiliation: University of California, Berkeley
    address: >
             130 Mulford Hall #3114,
             Berkeley, CA 94720-3114 USA 
    email: \email{cboettig@berkeley.edu}
    url: https://carlboettiger.info
  - name: Jorrit Poelen
    affiliation: Independently affiliated
title:
  formatted: "Content Identifiers for Reproducible Research using \\pkg{contentid}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Content Identifiers for Reproducible Research"
  # For running headers, if needed
  short:     "\\pkg{contentid}: Content Identifiers"
abstract: >
          Digital Object Identifiers (DOIs) have become the gold standard for referencing persistent,
          published research artifacts such as data files.  Unfortunately, DOIs are not well-designed
          for the use case of scripted analyses common in R and other languages. Instead of writing 
          the DOI directly into the script, researchers must usually rely on URLs or local file paths
          in the script, leaving the task of obtaining external data files required to run the analysis
          in the more fragile state that DOIs were orginally introduced to solve: namely, redirecting
          to the most recent location where the data may be found.  \\pkg{contentid} implements an 
          alternative approach in which an identifier is based on the cryptographic hash of the content
          itself rather than a location-based identifier.  While many other content-based storage systems 
          exist (version-control systems such as `git` being the most widely adopted), \\pkg{contentid}
          takes a simple and elegant approach based on *unsalted* content hashes which avoids locking the
          user into a specific tool or platform, and offers a powerful mechanism for interacting with
          metadata and provenance records of research data.  I summarize the package use, discuss the broader
          landscape of content-based identifier standards and tools and design considerations involved, and 
          then close with a few examples of more complex applications of the \\pkg{contentid} approach.  
keywords:
  # at least one keyword must be supplied
  formatted: [hash, uri, doi, identifier, sha256]
  plain:     [hash, uri, doi, identifier, sha256]
preamble: >
  \usepackage{amsmath}
output: rticles::jss_article
---

# Introduction

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
Sys.setenv("CONTENTID_HOME"= tempdir())

library(contentid)
```


## What is a content-based identifier?


## Why use content-based identifiers

DOIs do not refer to specific content.  In general, a DOI merely needs to redirect to the landing page of a persistent archive. Consider <>, the DOI for the NASA MODIS satellite data product, which refers to hundreds of thousands of individual data files updated hourly and distributed through a web interface or FTP server.  This is usually sufficient for a suitably knowledgeable human being to successfully locate and download the data, but not well suited for use in computer code which must know precisely which individual data files to download and process. 

A better way to reference specific content is to use the content's _cryptographic hash_, such as the SHA-256 checksum.  Checksums such as MD5, SHA-1, and SHA-256 are commonly used in data repositories to ensure data integrity -- that a file has not been corrupted due to slow degradation of the hardware such as the disks on which it is stored, or that some bits have not been lost or altered during a file download.  Such checksums are some of the most widely used algorithms in computing: universally recognized, widely implemented, and efficient.  Because it is also cryptographically secure, the SHA-256 checksum is so central that major chipmakers now build support for this method directly into the chip hardware, supported by assembly-based code written in major open source implementations such as openssl [@openssl]. Consequently, on recent processors the SHA-256 checksum is often significantly faster to compute then the less secure MD5 and SHA1 algorithms, especially on larger files.  

Many scientific data repositories already store and list checksum information, and even support searches for objects by their checksums.  This allows us to use checksums as natural identifiers for objects in these data repositories.  

Checksums have many advantages compared to alternative identifiers for individual data objects: they are (1) secure, (2) sticky, (3) portable (4) rot resistant, (5) cheap, (6) faciliate caching downloads, (7) facilitate caching workflows,  


R users frequently write scripts which must load data from an external file -- a step which increases friction in reuse and creates a common failure point in reproducibility of the analysis later on. Reading a file directly from a URL is often preferable, since we don't have to worry about distributing the data separately ourselves.  For example, an analysis might read in the famous CO2 ice core data directly from ORNL repository:

```{r}
co2 <- read.table("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/ess-dive-457358fdc81d3a5-20180726T203952542", 
                  col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

However, we know that data hosted at a given URL could change or disappear, and not all data we want to work with is available at a URL to begin with.  Digital Object Identifiers (DOIs) were created to deal with these problems of 'link rot'.  Unfortunately, there is no straight forward and general way to read data directly from a DOI, (which almost always resolves to a human-readable webpage rather than the data itself), often apply to collections of files rather than individual source we want to read in our script, and we must frequently work with data that does not (yet) have a DOI. Registering a DOI for a dataset has gotten easier through repositories with simple APIs like Zenodo and figshare, but this is still an involved process and still leaves us without a mechanism to directly access the data.  

`contentid` offers a complementary approach to addressing this challenge, which will work with data that has (or will later receive) a DOI, but also with arbitrary URLs or with local files. The basic idea is quite similar to referencing data by DOI: we first "register" an identifier, and then we use that identifier to retrieve the data in our scripts:

```{r}
register("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/ess-dive-457358fdc81d3a5-20180726T203952542")
```

Registering the data returns an identifier that we can `resolve` in our scripts to later read in the file:

```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
co2_b <- read.table(co2_file, 
                    col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

Note that we have manually embedded the identifier in our script, rather than automatically passing the identifier returned by `register()` directly to resolve.  The command to `register()` needs to only be run once, and thus doesn't need to be embedded in our script (though it is harmless to include it, as it will always return the same identifier unless the data file itself changes). 

We can confirm this is the same data:

```{r}
identical(co2, co2_b)
```

## How this works

As the identifier (`hash://sha256/...`) itself suggests, this is merely the SHA-256 hash of the requested file.  This means that unless the data at that URL changes, we will always get that same identifier back when we register that file.  If we have a copy of that data someplace else, we can verify it is indeed precisely the same data.  For instance, `contentid` includes a copy of this file as well.  Registering the local copy verifies that it indeed has the same hash:

```{r}
co2_file_c <- system.file("extdata", "vostok.icecore.co2", package = "contentid")
register(co2_file_c)
```

We have now registered the same content at two locations: a URL and a local file path.  `resolve()` will use this registry information to access the requested content. `resolve()` will choose a local path first, allowing us to avoid re-downloading any content we already have.  `resolve()` will verify the content of any local file or file downloaded from a URL matches the requested content hash before returning the path. If the file has been altered in any way, the hash will no longer match and `resolve()` will try the next source.  

We can get a better sense of this process by querying for all available sources for our requested content:

```{r results="hide"}
df <- query_sources("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
df
```

```{r echo=FALSE}
kableExtra::kable(df, "latex")
```

Note that `query_sources()` has found more locations than we have registered above.  This is because in addition to maintaining a local registry of sources, `contentid` registers online sources with the Hash Archive, <https://hash-archive.org>. (The Hash Archive doesn't store content, but only a list of public links at which content matching the hash has been seen.)  `query_sources()` has also checked for this content on the Software Heritage Archive, which does periodic crawls of all public content on GitHub which have also picked up a copy of this exact file. With each URL is a date at which it was last seen - repeated calls to `register()` will update this date, or lead to a source being deprecated for this content if the content it serves no longer matches the requested hash. We can view the history of all registrations of a given source using `query_history()`.  

This approach can also be used with local or unpublished data.  `register()`ing a local file only creates an entry in `contentid`'s local registry, so this does not provide a backup copy of the data or a mechanism to distribute it to collaborators.  But it does provide a check that the data has not accidentally changed on our disk. If we move the data or eventually publish the data, we have only to register these new locations and we never need to update a script that accesses the data using calls to `resolve()` like `read.table(resolve("hash://sha256/xxx..."))` rather than using local file names.  

If we prefer to keep a local copy of a specific dataset around, (e.g. for data that is used frequently or used across multiple projects), we can instruct `resolve()` to store a persistent copy in `contentid`'s local storage:

```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37", 
                    store = TRUE)
```

Any future calls to `resolve()` with this hash on this machine will then always be able to load the content from the local store.  This provides a convenient way to cache downloads for future use.  Because the local store is based on the content identifier, repeatedly storing the same content will have no effect, and we cannot easily overwrite or accidentally delete this content.


`register()` and `resolve()` provide a low-friction mechanism to create a permanent identifier for external files and then resolve that identifier to an appropriate source.  This can be useful in scripts that are frequently re-run as a way of caching the download step, and simultaneously helps ensure the script is more reproducible.  While this approach is not fail-proof (since all registered locations could fail to produce the content), if all else fails our script itself still contains a cryptographic fingerprint of the data we could use to verify if a given file was really the one used.  


## Acknowledgements

`contentid` is largely based on the design and implementation of <https://hash-archive.org>, and can interface with the <https://hash-archive.org> API or mimic it locally.  `contentid` also draws inspiration from [Preston](https://github.com/bio-guoda/preston), a biodiversity dataset tracker, and [Elton](https://github.com/globalbioticinteractions/elton), a command-line tool to update/clone, review and index existing species interaction datasets.




```{r include = FALSE}
Sys.unsetenv("CONTENTID_HOME")
```



# Use cases:

Citations: Citations are typically aggregated to the level of 'data packages' which may contain many objects (the DOIs used for products such as MODIS or NEON are an extreme example).  Later releases may contain files that are carried over from previous releases unchanged.  Data may also be re-used and re-published unchanged in later data packages in the process of analyzing previously released data files.  


Do I cite the most recent record (latest version), series identifier for the version (if it exists) the oldest record (original), most authoritative? Under current practices, no doubt researchers will already cite the "wrong" version, such as failing to notice the same data product appeared as part of an earlier record before being republished as part of a newer one.  When we rely on citations alone to understand data provenance, such cases are difficult to diagnose.  One of the powerful ideas of content identifiers is having a clear vocabulary to distinguish between content and concept. Given a list of data products containing the content, a citation to the content identifier, and an agreed-upon procedure for determining the authoritative citation, any software responsible for adding up citation metrics has all the information it needs to resolve a citation to the correct authority

Worflows: The use of content-based identifiers helps in large workflows in serveral ways. As discussed above, it (1) guarantee that the workflow only runs with identical data each time and (2) accelerates performance by avoiding re-downloading of content each time a workflow is run.  This approach is also important to more complex workflows where we can easily avoid re-computing expensive operations when relevant parts of a data analysis are unchanged.  One example is a forecasting workflow using NEON data.  NEON filenames frequently change without change to the underlying data.  A content-based system can avoid re-running parts of an analysis that would be re-executed under a location-based protocol [@neonstore]

Limitations:
Dynamic data, databases.  Data stored in dynamic structures and extracted on the fly can be difficult to operate on in a content-based structure.  However, it is important to note that such formats are also not considered best practice for long term archiving, as changes in software can render such data inaccessible.  Nor are these approaches required to achieve scale.  GBIF archives one large CSV.  NEON or MODIS examples instead rely on thousands of individual files.  This simplifies data transfer.  
Modern, high-performance systems like parquet are designed explicitly to take advantage of distributed file storage rather than a single continguous database file.  Such module approaches facilitates provenance tracing based on content hash.  


Schemes:


Downsides to hash URIs:

- "Hash URI" is already a term W3C uses to describe any URIs with a `#` sign: https://www.w3.org/wiki/HashURI
- https://github.com/hash-uri/hash-uri is not a formal proposed standard, like RFC6920 (named information)
- The "standard" is more ambiguous on certain issues.  For example, it does not define the list of acceptable names for hash algorithms, and in practice uses names that are not recognized by the IANA list and fails to recognize ones that are (i.e. `sha256` vs `sha2-256`).
- `hash://` prefix is not as friendly to URL-encoding.  


Downsides of alternatives:

- Many alternatives use base64-encoded strings.  Because base64 includes special characters such as `/`, these identifiers must aso accept percent-encoded strings (RFC6920 discusses this for `ni://`, in which percent encoding is optional on certain characters but not on others). This means that even within a given hash algorithm, we can have considerable variation in the literal string used as an identifier.  RFC6920 introduces other considerations as well, such as noting that padding character `=` should be removed.  Some implementations of the `ni://` spec fail to handle all of these cases, illustrating the challenges that come with the grreater complexity of base64-encoded identifiers.  

- Several of the alternative standard representations of a hash and the algorithm that produced it are not valid URIs, making them inappropriate for use in contexts that require URI-compliant identifiers, such as JSON-LD and other RDF documents. 


`contentid` understands other schemes and can translate between them in most cases.  

```{r}
contentid:::as_hashuri("ni:///sha256;lBIyWDHasiruvdZ0tutTumt73QS7maTbsh3f9kYofjc")
x <- resolve("ni:///sha256;lBIyWDHasiruvdZ0tutTumt73QS7maTbsh3f9kYofjc", registries = content_dir())
content_id(x)
```


