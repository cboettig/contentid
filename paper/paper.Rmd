---
documentclass: jss
author:
  - name: Carl Boettiger
    affiliation: University of California, Berkeley
    address: >
             130 Mulford Hall #3114,
             Berkeley, CA 94720-3114 USA 
    email: \email{cboettig@berkeley.edu}
    url: https://carlboettiger.info
  - name: Jorrit Poelen
    affiliation: Independently affiliated
title:
  formatted: "Content Identifiers for Reproducible Research using \\pkg{contentid}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Content Identifiers for Reproducible Research"
  # For running headers, if needed
  short:     "\\pkg{contentid}: Content Identifiers"
abstract: >
          Digital Object Identifiers (DOIs) have become the gold standard for referencing persistent,
          published research artifacts such as data files.  Unfortunately, DOIs are not well-designed
          for the use case of scripted analyses common in R and other languages. Instead of writing 
          the DOI directly into the script, researchers must usually rely on URLs or local file paths
          in the script, leaving the task of obtaining external data files required to run the analysis
          in the more fragile state that DOIs were orginally introduced to solve: namely, redirecting
          to the most recent location where the data may be found.  In this paper, we argue that 
          _content-based identifiers_ provide a robust and reliable mechanism for referencing archival
          data objects which is particularly well-suited for code and software applications. 
          Compared to DOIs, content-based identifiers are scripting-compatible, distributed, secure, 
          portable, cheap, offline-compatible, rot-resistant, cache-friendly and sticky.
          We discuss design considerations and existing use, and illustrate this approach through an
          implementation in our R package, \\pkg{contentid}.  
keywords:
  # at least one keyword must be supplied
  formatted: [hash, uri, doi, identifier, sha256]
  plain:     [hash, uri, doi, identifier, sha256]
preamble: >
  \usepackage{amsmath}
output: rticles::arxiv_article
---

# Introduction

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
Sys.unsetenv("CONTENTID_REGISTRIES")
Sys.setenv("CONTENTID_HOME"= tempdir())

library(contentid)
```


## What is a content-based identifier?

Content-based vs location-based

We typically think of data files through a location-based paradigm. A file location may be specified a Uniform Resource Locator (URL), such as <https://example.com/data.csv>, or a local relative or absolute file path, such as `path/to/data.csv` or `/home/user/path/to/data.csv`. Note that the file name, `data.csv`, can be considered an example of 'relative' file path. DOIs and other persistent identifiers (e.g. EZIDs, UUIDs) are typically used in a location-based manner as well: a central service "resolves" a DOI to a specific URL of a specific data repository. The use of the redirect makes it possible for that URL to be updated later, ameliorating the issue of link-rot [@fenner], but the identify of the file remains specified by it's location in a particular data repository or archive.  By contrast, content-based identifiers refer to files by the cryptographic checksums or hash.  A data file has the same checksum regardless of it's location: one of the primary uses of checksums has been to ensure that a file has not been altered during transfer from one location to another. Content-based-address systems such as `git` and `dropbox` store and retrieve files based on such checksums. Because every committed change to a file has a unique hash, this approach is particularly compelling for version control (`git`).  Because identical files have the same hash, this approach is also a natural choice when de-duplication is a priority (`dropbox`).  In both systems, the user is presented with a location-based interface, allowing a user to rely on location-based intuition for accessing files, while simultaneously being able to work with the same content across multiple locations or devices.  Content-based address systems are also a key component of distributed file servers such as torrents [@torrent].  The success of such platforms has also led to various initiatives to provide "`git` for data" [@IPFS; @dat].  This paper is not a proposal for building such a platform, but rather, seeks to examine design principles which allow any such approach to efficient, interoperable, and compatible with existing data archiving systems.  We illustrate how a content-based identifier approach can be incorporated into daily scripts in place of local paths or URLs, providing robust and reliable access to data using an easy-to-generate persistent identifier that can follow a data product from the moment it exists in digital form through preliminary distribution to  publication in a data archive.  




From @hash-uri


The hash URI scheme follows [RFC 3986](https://tools.ietf.org/html/rfc3986).

```
 hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37?type=text/plain#top
 \__/   \____/ \______________________________________________________________/ \_____________/ \_/
  |       |               |                                                            |         |
scheme algorithm         hash                                                        query    fragment
```

Scheme: The fixed string "hash".

Algorithm: A hash algorithm name, in the form of a pseudo-DNS name. Acceptable characters are alpha-numerics, `.` (period) and `-` (hyphen). Dotted segments must have at least one character and cannot begin or end with hyphens.

Hash: The hash of the resource, using the given algorithm. Currently must be encoded in hexadecimal (base-64 support is planned). Can be truncated to any non-zero number of characters, although this may lead to ambiguity in resolvers. Hex encoding is case-insensitive but other encodings may be case-sensitive.

Query: Query parameters, which are interpreted by the resolver. Since a hash URI can be resolved by different systems, query parameters must be semi-standardized.

Fragment: Indicates a sub-resource, if any.


$$\underbrace{\texttt{hash://}}_{\textrm{scheme}}\underbrace{\texttt{sha256/}}_{\textrm{algorithm}}\underbrace{\texttt{9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37}}_{\textrm{hash}}\underbrace{\texttt{?type=text/plain}}_{\textrm{query (optional metadata)}}$$


## Why use content-based identifiers


- Content-based identifiers are **scripting compatible**.

DOIs do not refer to specific content, which makes them difficult to use directly in scripts and software packages.   In general, a DOI merely needs to redirect to the landing page of a persistent archive. Consider <doi:10.5067/MODIS/MOD14A1.006>, the DOI for the NASA MODIS satellite data product for "Thermal Anomalies and Fire Daily (MOD14A1)  Version 6", which includes hundreds of thousands of individual data files updated daily and distributed through a web interface or FTP server.  The DOI is sufficient for a suitably knowledgeable human being to successfully locate and download the data, but not well suited for use in computer code which must know precisely which individual data files to download and process. Even most DOIs that resolve to permanent and immutable data objects cannot reliably resolved by computer code to find the download URLs for the actual content. DOIs are principly designed for humans, not computers.  How then can we reliably reference and retrieve specific archival data in scientific code scripts and software packages?

- Content-based identifiers are **distributed**.

A central premise of any digital data preservation strategy is the LOCKSS principle: "Lots of copies keeps stuff safe." 
For example, DOI-granting academic journals typically[^1] participate in partnerships such as CLOCKSS (Controlled LOCKSS, @CLOCKSS) in which members duplicate or mirror content from other participants.
The DataONE repository network takes a similar approach in which member data repositories mirror some content from the other repositories.
These approaches rely on a centralized service or coordinating node that can resolve a request for particular content to the appropriate location, which still creates a single point of failure.
Content-based identifiers allow a similar approach to distributed storage through a more fully de-centralized approach.
Research data files are already frequently found at multiple locations: a local hard-drive, a university server, a cloud storage provider, a GitHub repository, or a formal data archive.
Any user can construct a look-up table of content-based identifiers and the sources (URLs or local file paths) at which the corresponding content has been found.
Note that because we can always compare the identifier with the checksum signature of the currently found at a given location, we have cryptographic certainty that it is the desired content.
We refer to such a look-up table as a content "registry," which may also list other relevant information about the source, such as the date at which the content was found.
Any of these locations may be subject to link rot, in which content changes or moves.
In such cases, those sources will no longer be able to produce content with a hash matching the requested identifier, signalling that we will have to try an alternative source from the registry.
By itself, this approach does not necessarily guarantee long-term redundant storage: the registry only points to other storage systems.
This contrasts with the standard approach of scientific repositories, which only issue permanent object identifiers for content in their own storage system.
Decoupling the roles of "resolving" and identifier to content vs "storing" the content provides additional flexibility that can be very powerful.
Because most data repositories already compute and store checksums for the data files they contain (a neccessary element to ensure archival data is not corrupted), data repositories are natural registries of their own content - able to map identifiers to download URLs. 
A decentralized registry approach immediately allows us to extend this strategy to accomodate data that is not (or not yet) in a permanent data archive, and also provides a mechanism to refer to data in multiple locations without a central coordinating node.
Because the content-identifier ensures the content is perfectly identical, we are also free to choose the most convenient source, such as a locally-hosted file, rather than relying on a download from a trusted repository to ensure authenticity. 


- Content-based identifiers are secure by default

Using a content-based identifier ensures that data is not accidentally changed or altered. Local paths can easily be overwritten, and files at a given URL updated.  Sometimes this is desirable, but when reproducibility is required, referencing data by content identifier avoids this risk.  While chance collisions with MD5 are SHA-1 are extremely unlikely, it is possible at least in principle to generate collisions in which altered content has the same MD5 or SHA-1 sum as the desired content [@collisions]. By contrast, SHA-256 hashes are considered cytographically secure at this time, giving a robust assurance that the data has not been altered.  Traditionally, users are encouraged to manually verify the checksum of any data downloaded from a DOI or URL to ensure that the data has not been altered (either maliciously or due to packet loss) during transmission.  In practice this extra step may be uncommon, as it requires additional effort and many data repositories do not clearly display the checksum in the first place.  Using a content-based identifier allows us to build in such verification to the download process by default.  For example, the `resolve()` function automatically verifies that the downloaded object matches the checksum specified in the identifier. 


```{r}
```

- Content-based identifiers are portable

- Content-based identifiers are cheap and easy to produce, even offline.

Generating a DOI can only be done with the help of an internationally recognized DOI-granting repository. The user or the repository must typically pay a nominal fee associated in minting the DOI, and must register minimal metadata required to generate a citation to the deposited object(s) with the central authority (DataCite for data-DOIs) in exchange for the DOI. Users or software interacting with the repository thus need to authenticate a user. In contrast, open source content hash algorithms are free and widely available on almost all computer platforms.  Algorithms such as MD5, SHA-1, and SHA-256 are some of the most widely used, studied and implemented on the planet, and thus least likely to be lost to future generations.  As evidence of this centrality, the SHA-2 family of checksums, including SHA-256 are so central to modern computing that major chipmakers now build support for this method directly into the chip hardware, supported by assembly-based code written in major open source implementations such as openssl [@openssl]. Consequently, on recent processors the SHA-256 checksum is often significantly faster to compute then the less secure MD5 and SHA1 algorithms, especially on larger files.  This should alleviate the primary objection most data archives have to adopting the more secure SHA-256 in place of MD5 or SHA1.  Because minting a DOI for data requires communication with a registered repository, this typically requires online access.  In contrast, a content-based identifier can be minted offline. 


- Content-based identifiers are cache-friendly.

A major advantage of content-based identifiers in scripts is the ability to avoid re-downloading data that has already been downloaded locally.  This efficiency is particularly important to debugging scripts or automating workflows, where the same code may be run repeatedly.  By maintaining a local content-addressed storage cache, software can determine if the requested identifier already exists locally before attempting to resolve the identifier to a registered data repository or URL.  This allows us to ensure that our code can run successfully whether or not the data has been downloaded.



```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37", 
                    store = TRUE)
```



- Content-based identifiers are metadata-friendly

A Uniform Resource Identifier (URI) is the gold standard for metadata objects to refer
to the data they document.  The Resource Description Framework (RDF) requires URI-based
identifiers.  DOIs are an example of such a URI, but DOIs typically resolve to HTML
landing pages and thus do not identify the various data objects precisely.  A URL is also
a URI, but may not always resolve to the same content, and the URL may not be known at the 
time the metadata is generated.  Universal Unique Identifiers (UUIDs) are another common
choice, with an efficient standard algorithm.  Unfortunately, this approach can generate
different identifiers for the same data object, which can be particularly confusing when
the metadata and UUID are being generated directly by scripts.  Metadata systems also often
refer to objects using identifiers that are not globally unique, such as filenames or
id numbers.  


- Content-based identifiers are sticky

While scientific papers typically print their DOIs in the article PDF directly, it is usually impossible to know the identifier for a data file from inspecting the data itself.
Instead, the identifier is typically recorded in a separate metadata file, making it possible for the identifier information to be become separated from the data file itself.
Using content hashes, the content becomes it's own identifier.
As long as we have the data file, we can easily determine the identifier by re-computing the appropriate checksum.
This also means that there is never any need to 'pre-register' or 'reserve' the identifier, the moment the data exists in a digital serialization, it has a unique identifier we can use to refer to it. 

- Content-based identifiers help with de-duplication



- Content-based identifiers are repository-friendly

Many scientific repositories already record 



A better way to reference specific content is to use the content's _cryptographic hash_, such as the SHA-256 checksum.  Checksums such as MD5, SHA-1, and SHA-256 are commonly used in data repositories to ensure data integrity -- that a file has not been corrupted due to slow degradation of the hardware such as the disks on which it is stored, or that some bits have not been lost or altered during a file download.  Such checksums are some of the most widely used algorithms in computing: universally recognized, widely implemented, and efficient.  Because it is also cryptographically secure, 

Many scientific data repositories already store and list checksum information, and even support searches for objects by their checksums.  This allows us to use checksums as natural identifiers for objects in these data repositories.  

Checksums have many advantages compared to alternative identifiers for individual data objects: they are (1) secure, (2) sticky, (3) portable (4) rot resistant, (5) cheap, (6) faciliate caching downloads, (7) facilitate caching workflows,  


R users frequently write scripts which must load data from an external file -- a step which increases friction in reuse and creates a common failure point in reproducibility of the analysis later on. Reading a file directly from a URL is often preferable, since we don't have to worry about distributing the data separately ourselves.  For example, an analysis might read in the famous CO2 ice core data directly from ORNL repository:

```{r}
co2 <- read.table("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/ess-dive-457358fdc81d3a5-20180726T203952542", 
                  col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

However, we know that data hosted at a given URL could change or disappear, and not all data we want to work with is available at a URL to begin with.  Digital Object Identifiers (DOIs) were created to deal with these problems of 'link rot'.  Unfortunately, there is no straight forward and general way to read data directly from a DOI, (which almost always resolves to a human-readable webpage rather than the data itself), often apply to collections of files rather than individual source we want to read in our script, and we must frequently work with data that does not (yet) have a DOI. Registering a DOI for a dataset has gotten easier through repositories with simple APIs like Zenodo and figshare, but this is still an involved process and still leaves us without a mechanism to directly access the data.  

`contentid` offers a complementary approach to addressing this challenge, which will work with data that has (or will later receive) a DOI, but also with arbitrary URLs or with local files. The basic idea is quite similar to referencing data by DOI: we first "register" an identifier, and then we use that identifier to retrieve the data in our scripts:

```{r}
register("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/ess-dive-457358fdc81d3a5-20180726T203952542")
```

Registering the data returns an identifier that we can `resolve` in our scripts to later read in the file:

```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
co2_b <- read.table(co2_file, 
                    col.names = c("depth", "age_ice", "age_air", "co2"), skip = 21)
```

Note that we have manually embedded the identifier in our script, rather than automatically passing the identifier returned by `register()` directly to resolve.  The command to `register()` needs to only be run once, and thus doesn't need to be embedded in our script (though it is harmless to include it, as it will always return the same identifier unless the data file itself changes). 

We can confirm this is the same data:

```{r}
identical(co2, co2_b)
```

## How this works

As the identifier (`hash://sha256/...`) itself suggests, this is merely the SHA-256 hash of the requested file.  This means that unless the data at that URL changes, we will always get that same identifier back when we register that file.  If we have a copy of that data someplace else, we can verify it is indeed precisely the same data.  For instance, `contentid` includes a copy of this file as well.  Registering the local copy verifies that it indeed has the same hash:

```{r}
co2_file_c <- system.file("extdata", "vostok.icecore.co2", package = "contentid")
register(co2_file_c)
```

We have now registered the same content at two locations: a URL and a local file path.  `resolve()` will use this registry information to access the requested content. `resolve()` will choose a local path first, allowing us to avoid re-downloading any content we already have.  `resolve()` will verify the content of any local file or file downloaded from a URL matches the requested content hash before returning the path. If the file has been altered in any way, the hash will no longer match and `resolve()` will try the next source.  

We can get a better sense of this process by querying for all available sources for our requested content:

```{r results="hide"}
df <- query_sources("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37")
df
```

```{r echo=FALSE}
kableExtra::kable(df, "latex")
```

Note that `query_sources()` has found more locations than we have registered above.  This is because in addition to maintaining a local registry of sources, `contentid` registers online sources with the Hash Archive, <https://hash-archive.org>. (The Hash Archive doesn't store content, but only a list of public links at which content matching the hash has been seen.)  `query_sources()` has also checked for this content on the Software Heritage Archive, which does periodic crawls of all public content on GitHub which have also picked up a copy of this exact file. With each URL is a date at which it was last seen - repeated calls to `register()` will update this date, or lead to a source being deprecated for this content if the content it serves no longer matches the requested hash. We can view the history of all registrations of a given source using `query_history()`.  

This approach can also be used with local or unpublished data.  `register()`ing a local file only creates an entry in `contentid`'s local registry, so this does not provide a backup copy of the data or a mechanism to distribute it to collaborators.  But it does provide a check that the data has not accidentally changed on our disk. If we move the data or eventually publish the data, we have only to register these new locations and we never need to update a script that accesses the data using calls to `resolve()` like `read.table(resolve("hash://sha256/xxx..."))` rather than using local file names.  

If we prefer to keep a local copy of a specific dataset around, (e.g. for data that is used frequently or used across multiple projects), we can instruct `resolve()` to store a persistent copy in `contentid`'s local storage:

```{r}
co2_file <- resolve("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37", 
                    store = TRUE)
```

Any future calls to `resolve()` with this hash on this machine will then always be able to load the content from the local store.  This provides a convenient way to cache downloads for future use.  Because the local store is based on the content identifier, repeatedly storing the same content will have no effect, and we cannot easily overwrite or accidentally delete this content.


`register()` and `resolve()` provide a low-friction mechanism to create a permanent identifier for external files and then resolve that identifier to an appropriate source.  This can be useful in scripts that are frequently re-run as a way of caching the download step, and simultaneously helps ensure the script is more reproducible.  While this approach is not fail-proof (since all registered locations could fail to produce the content), if all else fails our script itself still contains a cryptographic fingerprint of the data we could use to verify if a given file was really the one used.  


## Acknowledgements

`contentid` is influenced by design and implementation of <https://hash-archive.org>, and can interface with the <https://hash-archive.org> API and mimic that functionality locally.  `contentid` also draws inspiration from [Preston](https://github.com/bio-guoda/preston), a biodiversity dataset tracker, and [Elton](https://github.com/globalbioticinteractions/elton), a command-line tool to update/clone, review and index existing species interaction datasets.





# Use cases:

Citations: Citations are typically aggregated to the level of 'data packages' which may contain many objects (the DOIs used for products such as MODIS or NEON are an extreme example).  Later releases may contain files that are carried over from previous releases unchanged.  Data may also be re-used and re-published unchanged in later data packages in the process of analyzing previously released data files.  


Do I cite the most recent record (latest version), series identifier for the version (if it exists) the oldest record (original), most authoritative? Under current practices, no doubt researchers will already cite the "wrong" version, such as failing to notice the same data product appeared as part of an earlier record before being republished as part of a newer one.  When we rely on citations alone to understand data provenance, such cases are difficult to diagnose.  One of the powerful ideas of content identifiers is having a clear vocabulary to distinguish between content and concept. Given a list of data products containing the content, a citation to the content identifier, and an agreed-upon procedure for determining the authoritative citation, any software responsible for adding up citation metrics has all the information it needs to resolve a citation to the correct authority

Worflows: The use of content-based identifiers helps in large workflows in serveral ways. As discussed above, it (1) guarantee that the workflow only runs with identical data each time and (2) accelerates performance by avoiding re-downloading of content each time a workflow is run.  This approach is also important to more complex workflows where we can easily avoid re-computing expensive operations when relevant parts of a data analysis are unchanged.  One example is a forecasting workflow using NEON data.  NEON filenames frequently change without change to the underlying data.  A content-based system can avoid re-running parts of an analysis that would be re-executed under a location-based protocol [@neonstore]

Limitations:
Dynamic data, databases.  Data stored in dynamic structures and extracted on the fly can be difficult to operate on in a content-based structure.  However, it is important to note that such formats are also not considered best practice for long term archiving, as changes in software can render such data inaccessible.  Nor are these approaches required to achieve scale.  GBIF archives one large CSV.  NEON or MODIS examples instead rely on thousands of individual files.  This simplifies data transfer.  
Modern, high-performance systems like parquet are designed explicitly to take advantage of distributed file storage rather than a single continguous database file.  Such module approaches facilitates provenance tracing based on content hash.  


Schemes:


Downsides to hash URIs:

- "Hash URI" is already a term W3C uses to describe any URIs with a `#` sign: https://www.w3.org/wiki/HashURI
- https://github.com/hash-uri/hash-uri is not a formal proposed standard, like RFC6920 (named information)
- The "standard" is more ambiguous on certain issues.  For example, it does not define the list of acceptable names for hash algorithms, and in practice uses names that are not recognized by the IANA list and fails to recognize ones that are (i.e. `sha256` vs `sha2-256`).
- `hash://` prefix is not as friendly to URL-encoding.  


Downsides of alternatives:

- Many alternatives use base64-encoded strings.  Because base64 includes special characters such as `/`, these identifiers must aso accept percent-encoded strings (RFC6920 discusses this for `ni://`, in which percent encoding is optional on certain characters but not on others). This means that even within a given hash algorithm, we can have considerable variation in the literal string used as an identifier.  RFC6920 introduces other considerations as well, such as noting that padding character `=` should be removed.  Some implementations of the `ni://` spec fail to handle all of these cases, illustrating the challenges that come with the grreater complexity of base64-encoded identifiers.  

- Several of the alternative standard representations of a hash and the algorithm that produced it are not valid URIs, making them inappropriate for use in contexts that require URI-compliant identifiers, such as JSON-LD and other RDF documents. 


`contentid` understands other schemes and can translate between them in most cases.  

```{r}
contentid:::as_hashuri("ni:///sha256;lBIyWDHasiruvdZ0tutTumt73QS7maTbsh3f9kYofjc")
x <- resolve("ni:///sha256;lBIyWDHasiruvdZ0tutTumt73QS7maTbsh3f9kYofjc", registries = content_dir())
content_id(x)
```



## Registries in `contentid`

The `contentid` package can generate and maintain a simple registry of sources for content using a local plain text file in tab-separated-values (tsv) format, or local Lightning Memory-Mapped Database (LMDB).
The latter will offer the best performance for large registries (containing millions or more identifiers).
These local registeries can themselves be archived and distributed.  `contentid` uses an extensible model which allows it to access an arbitrary number of separate tsv files and/or LMDB databases.  
A more efficient mechanism of making such registeries available to others is provided by <https://hash-archive.org>, a project of the Internet Archive.  Hash Archive is a small server platform which can take either a URL or a content-based identifier as input.  
Given a URL, Hash Archive streams the data, computing the MD5, SHA1, SHA-256, SHA-384, and SHA-512 checksums and storing this information in its local registry, along with the corresponding URL, timestamp, and file size information.  
Given a content identifier as input, Hash Archive returns a list of a URLs which have been previously registered matching that hash. 
`contentid` also treats several major data repositories as implicit registeries of their own content, including Zenodo, the DataONE Repository Network (with over forty member repositories, including Dryad, CDIAC, and EFI), and the Software Heritage Project.  
Unfortunately, data repositories differ in their choice(s) of checksum. For rexample, at this time, SoftwareHeritage uses SHA-256, Zenodo uses MD5, and DataONE member repositories either choose or allow individual researchers depositing data to select their checksum algorithm.


```{r results="hide"}
df <- query_sources("hash://sha256/9412325831dab22aeebdd674b6eb53ba6b7bdd04bb99a4dbb21ddff646287e37", cols = c("source", "date"))
df
```

```{r echo=FALSE}
kableExtra::kable(df, "latex")
```



















```{r teardown, include = FALSE}
Sys.unsetenv("CONTENTID_HOME")
```
