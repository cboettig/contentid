---
title: "Identifiers for Iterative Forecasting"
author: "Carl Boettiger Jorritt Poelen, Matthew Jones, Quinn Thomas"
bibliography: "refs.bib"
---



## Forecast Identifiers

This document proposes the use of content-based identifiers for publishing products associated with automated, iterative forecasts.  

Iterative forecasting will frequently involve automatically running code which ingests public data products and generates output forecast products, along with associated metadata.  Consequently, the forecasts produced may depend on the code and software which defines the forecast algorithm, as well as the input data used. Researchers must be able to uniquely identify and access each forecast generated by running the algorithm, as well as the associated input data files and code. 

## Proposed approach

We propose that forecast products be identified by their SHA-256 checksum in the [Hash-URI format](https://hash-archive.org):

```
hash://sha256/<HASH>
```

Note that this is an _un-salted_ hash, containing no additional metadata beyond the pure file hash (in contrast to other content-based storage systems such as `dat` or `IPFS`). Consequently, the URI tells us everything we need to know to generate the hash (i.e. the algorithm used is a sha256 hash).  For example, we can create an identifier for the `csv` serialization of popular example dataset, `mtcars`, [@mtcars] in R [@R] as follows:

```{r}
readr::write_csv(mtcars, "mtcars.csv")
hash <- openssl::sha256(file("mtcars.csv"))
paste0("hash://sha256/", hash)
```

Here we have used the `openssl` package's implementation of the sha256 algorithm, which binds a fast and widely used C library [@openssl].  Many other implementations are readily available (e.g. the `digest` package in R, [@digest], `sha256sum` [@gnucoreutils] ), and will produce the identical hash. 



## Rationale

1. Any copy of the same file, the same bits and bytes, will result in the same identifier. Using a strong cryptographic hash such as sha256 ensures that even a malicious actor cannot change the file in question without altering the identifier as well.
1. This means that content-based identifiers are _location agnostic_.  The content has the same identifier, regardless of whether it exists in a permanent archival repository or only on a single user's laptop.  This contrasts from location-based identifiers, such as a URLs.  Digital Object Identifiers (DOIs) are also location-based identifiers, because the DOI can resolve only to the specific repository hosting the content.
1. Content-based identifiers are ideal for distributed storage because they can be resolved to multiple locations.  A DOI or other location-based identifier can be resolved to only a single location (URL) at a time (e.g. in the case of DOIs, by the `https://doi.org` resolution service), even though most robust archival storage requires backup copies of content be stored in other archives (e.g. DataONE network for data, or the LOCKSS or CLOCKSS networks used by scientific publishers).  The Hash Archive, <https://hash-archive.org>, provides a service similar to the `https://doi.org` resolver for content-based identifiers, but instead return all registered locations.  This same property of content-based identifiers underlies other distributed storage algorithms such as "torrents."  Note that this is not a replacement for archival storage repositories: ideally at least one registered location corresponds to an archival repository for any data that needs to be permanently archived.
1. Content-based identifiers work well with data that is stored locally, stored on a non-archival (public or private) access point such as a GitHub repository or S3 bucket, or stored in any permanent scientific data archive (or all of the above simultaneously).  
1. These identifiers are easy to generate in scripted workflows.  Identifiers issued by a repository such as a DOI or other unique identifier (e.g. from Open Science Framework, OSF) which requires an available network connection for authenticating with a specific remote provider (e.g. using authentication tokens in the script which must be kept secure), and the latency involved in such communication.[^1]
1. These identifiers do not change if a script is re-run and produces the identical results and outputs.  This is not true of scripts which automatically register identifiers with remote services or with other unique identifier algorithms that can be run locally, such as UUIDs [@uuid].  UUIDs include information such as a timestamp which ensure that the identifier is different every time it is generated.  Iterative forecasts, by contrast, may be run and re-run many times to test code, verify reproducibility, or as part of continuous integration (CI) framework.  
1. Content-based identifiers unambiguously identify specific content.  Other identifiers such as DOIs may refer to a specific file, a collection of files, or even (as in the case of major remote sensing products) a general notion of a 'product' which contains thousands of component files which are continuously added and updated.  Conversely, a single file can be identified by multiple different identifiers, even multiple DOIs.  However,this is also a limitation of content-based identifiers: researchers often need identifiers to represent abstract concepts such as a "series identifier" which corresponds to the whole series of iterative forecasts, regardless of how many versions it contains. For abstract concepts, other identifiers are necessary.
1. Content-based identifiers are also easy to resolve in scripted workflows, because they can only resolve directly to their content.  A DOI typically resolves to a HTML landing page, which provides a human-readable description of where to download the data, but lacks a consistent machine-readable mechanism.  Programmatic access typically relies on an API that is specific to the repository.  
1. Content-based identifiers facilliate local caching of data files, which avoids repeated downloads in an automated workflow.  A script can easily confirm (with cryptographic certainty) that the desired content has already been downloaded locally, and then read the local copy instead of re-downloading from an authorative location.
1. Content-based identifiers cannot become 'unstuck' from the content they identify. Typically, identifiers are stored separately in metadata records, which map the identifier to a particular location (e.g. a relative path in a directory, or a location in a permanent archive.) Consequently, it can be difficult to confirm that a given file corresponds to the desired identifier. In contrast, as long as you have the data file, you can always calculate the `sha256` identifier for it. 
1. Other identifiers frequently face a "chicken-and-egg' problem in automated workflows.  This problem usually arises in attempt to address the previous problem of 'unstuck' identifiers, it is common practice to embed the identifier into the product itself (for example, most journal articles display their DOI on the first page, and many data packages include metadata files which state the identifier.)  This requires a two-stage workflow in which the script must first 'pre-register' or 'reserve' and identifier, and then embed that identifier in the data file prior to uploading the data to a repository.  Additional logic is required to either reserve a new identifier if the output product has changed, or avoid doing so if it has not.  
1. Content-based identifiers permit a phased approach that is ideal for developing and testing a workflow before it is ready to be put into production.  Few data repositories offer "testing" servers (the DataONE API is a notable exception) where a workflow that registers and uploads data can be run many times in testing without creating a permanent archive a lot of junk.  A script using content-based identifiers can generate, register, and resolve such identifiers locally without ever making them public.  When the researchers are satisfied with the script running locally, they can place a copy of the same data at any public location (university server, AWS S3 bucket, GitHub) and register that location, enabling collaborators to also resolve the files.  When a workflow is finally deemed ready to begin generating permanent archives, the script only need be extended to upload and register the location of the permanent archive.   

[^1]: Only for very large files do cryptographically strong algorithms such as `sha256` require non-negligible computational effort (e.g. the hash of a 10 GB file takes less than a minute on a laptop machine), and will in any event represent a small fraction of computational effort required for actual analysis of the fie.


## Example workflows

To facilitate the use of content-based identifiers, we provide a simple R package implementation, `contentid`.  An identifier can be calculated using `content_id`, which calculates the sha256 hash for the requested files (or URLs):

```{r}
library(contentid)
content_id("mtcars.csv")
```
At the start of develomment in an experimental workflow, it may be valuable to be able to "resolve" this identifier back to the location of said data file. To do so, we must first  "register" that this identifier corresponds to content found at this location: 

```{r}
id <- register("mtcars.csv", registries = content_dir())
```



# References

